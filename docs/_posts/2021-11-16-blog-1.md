---
title: Get the Data - Bicycle Volume & Speed Dataset
date: 2021-11-16 00:00:00
description: Welcome to the inaugural blog post! This post will cover how we imported the dataset into a python coding environment, and readied the dataframe for analysis and visualisation.
featured_image: '/images/Sensor_Site_Map.png'
---

### The dataset

Welcome to the inaugural blog post! This post will cover how we imported the dataset into a python coding environment, and readied the dataframe for analysis and visualisation.

Before starting, we've downloaded the bicycle volume and speed data from the DataVic site [here](https://discover.data.vic.gov.au/dataset/bicycle-volume-and-speed)



The data shows cycling flows in both directions recorded at 42 off-road counter sites and 4 on-road counter sites in Melbourne.

The dataset is delivered in a large number of CSV files, with each file containing several days worth of data from one sensor location. There's also an excel file called 'VicRoads_Bike_Site_Number_Listing' which indexes the location of each of the sensors. 

While the files could be programmatically unzipped with code, we unzipped them all using 7-zip in one big batch, so all of the CSVs were ready to go.

*By the way, there are already some great examples of how this dataset can be used â€” if you're curious, check out the VicRoads dashboard [here](https://app.powerbi.com/view?r=eyJrIjoiNzZiNTUxOTEtNmRhZC00YmMzLWI3ZGEtZDc4YTczNjg5NzE0IiwidCI6IjUwOTRjN2E3LTA3NDgtNDY2ZS05NDFlLTcyODgyYzMwOTdiYSJ9)*



### The code

We'll start off by importing all the python libraries that we'll need in the one spot: 

{% highlight py%}
import pandas as pd
import numpy as np
import os
from collections import Counter
from tqdm.notebook import tqdm
import plotly.express as px
import geopandas as gpd
import plotly.graph_objects as go
{% endhighlight %}
<p>&nbsp;</p> 

Then, let's create a couple of variables for the operating system file paths to access the CSVs downloaded from DataVic. This should save time later on...

We'll also limit our data to the last 5 years, from 2017 to 2021

{% highlight py%}
rootdir = '../../data/DPC/bicycle_flows/'
visuals = '../docs/visuals/bicycle_flows/'
data_years = [2017,2018,2019,2020,2021]
folder_prefix = 'Bicycle_Volume_Speed_'
{% endhighlight %}

### Data validation

There's a lot of CSV files, so lets first make sure that each CSV has the same variables or columns. Once this is confirmed, we'll be able to combine all of the individual files into a dataframe.

The code below iterates over each of the CSVs and tallies the number of unique column combinations. Ideally, the result will be a single combination of columns, so we'll be able to append each to a new dataframe. Let's see...

<p>&nbsp;</p>
{% highlight py %}
for year in data_years:
    tally = pd.DataFrame(columns=["count","len"])
    for subdir, dirs, files in tqdm(os.walk(rootdir)):
        for file in files:
            if ('.csv' in file) and ('.zip' not in file):
            # read in CSV, if it contains records 
                file_path = os.path.join(subdir,file) 
                if os.path.getsize(file_path) > 0:
                    df = pd.read_csv(os.path.join(subdir,file))
                    # store list of columns as a string
                    df_columns = f"{df.columns.to_list()}"
                    # if CSV columns string is in tally index, increment
                    if df_columns in tally.index:
                        tally[tally.index==df_columns] += 1
                        # otherwise add CSV columns string to tally index
                    else:
                        tally.loc[df_columns] = 1
                        
tally
{% endhighlight %}
<p>&nbsp;</p>
And the answer returned is only 1 unique combination of variables (across 16,054 CSV files), great!

|                                                                                                                                                                                                | count |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|------:|
| ['DATA_TYPE', 'TIS_DATA_REQUEST', 'SITE_XN_ROUTE', 'LOC_LEG', 'DATE', 'TIME', 'CLASS', 'LANE', 'SPEED', 'WHEELBASE', 'HEADWAY', 'GAP', 'AXLE', 'AXLE_GROUPING', 'RHO', 'VEHICLE', 'DIRECTION'] | 16054 |

### Creating a dataframe

Now that we know that all of the CSVs share the same column names, we'll join the various files together to create a dataframe to run the analysis.

We'll start by creating an empty dataframe list to put the data into, and then we'll 'go for a walk' through the operating system to pick up the files we want. We'll make sure we're only collecting files that have data (ie, are larger than 0)

*TQDM is a wrapper which creates a processing progress bar - useful when there's a lot of files...*

{% highlight py %}

df = []
for year in tqdm(data_years, unit = 'years'):
    csv_files = []
    for subdir, 
        dirs, 
        files in tqdm(os.walk(f'{rootdir}/{folder_prefix}{year}'),
                    desc= f"Getting CSV file paths ({year})...",
                    unit="CSVs",
                    leave = False):
        for file in files:
            if ('.csv' in file) and ('.zip' not in file):
                # record filepaths of CSVs containing records
                file_path = os.path.join(subdir,file)
                # let's make sure we only join files that contain data
                if os.path.getsize(file_path) > 0:
                    csv_files.append(os.path.join(subdir,file))
    dfs=[]
    for csv in tqdm(csv_files,
                    desc=f"Reading csv files for {year}...",
                    unit="CSVs",
                    leave = False):
        dfs.append(pd.read_csv(csv, index_col=None, header=0))
    
    # We'll create a variable to aggregate daily data as 'month_year'
    dfs = pd.concat(dfs, axis=0, ignore_index=True)
    dfs['month_year'] = pd.to_datetime(dfs['DATE'],
                                    format='%d/%m/%Y').dt.to_period('M')
    df.append(dfs[dfs.month_year.dt.year==year]\
              .groupby(['SITE_XN_ROUTE', 
                        'LOC_LEG',
                        'month_year'])[['VEHICLE', 'SPEED','HEADWAY']]\
              .agg({'VEHICLE': 'count',
                    'SPEED': 'mean',
                    'HEADWAY': 'mean'}).reset_index())
    del dfs

df = pd.concat(df, axis=0, ignore_index=True)

{% endhighlight %}
<p>&nbsp;</p>

Next, we'll write the dataframe to a CSV file so we won't need to rerun the previous cell if the kernel is stopped at any point.

{% highlight py %}
df.to_csv('combined_bicycle_summary_data.csv',index=False,header=True)
{% endhighlight %}

### Summary statistics

Let's look at the summary statistics for the dataframe. Firstly we'll look at what type of data is included in each column.

Then, we'll check the max/min values, and the distribution of the data, to see if there are any outlying data points.

{% highlight py %}
df.info()
{% endhighlight %}

RangeIndex: 4564 entries, 0 to 4563

|        Column | Non-Null Count |     Dtype |
|--------------:|---------------:|----------:|
| SITE_XN_ROUTE |  4564 non-null |     int64 |
|       LOC_LEG |  4564 non-null |     int64 |
|    month_year |  4564 non-null | period[M] |
|       VEHICLE |  4564 non-null |     int64 |
|         SPEED |  4564 non-null |   float64 |
|       HEADWAY |  4532 non-null |   float64 |

It looks like we might have some null entries in the HEADWAY column, so we'll keep an eye on that in upcoming analyses.

{% highlight py %}
df.describe().round({"LOC_LEG":1,"SPEED":1,"HEADWAY":1})\
  .astype({"SITE_XN_ROUTE": int, "VEHICLE": int})
{% endhighlight %}

|       | VEHICLE |  SPEED | HEADWAY |
|------:|--------:|-------:|--------:|
|  mean |   12132 |   20.8 |  1092.3 |
|   std |   10918 |    4.1 |  4460.7 |
|   min |       1 |    2.0 |    11.6 |
|   25% |    3722 |   17.9 |   133.8 |
|   50% |    9800 |   20.8 |   216.9 |
|   75% |   16907 |   23.4 |   458.5 |
|   max |  135024 |   33.8 | 69495.9 |

For average monthly bicycle speed, the median (50th percentile) & the mean share the same value. This suggests a relatively balanced distribution of data, with just as many bicycles travelling at below as above 20.8 km/hour. One sensor had a single bicycle cross it in a month - perhaps it was newly installed?

### Joining the sensor site index with the sensor data files

Now we that we have imported 5 years worth of sensor CSVs, let's join this dataframe to the site index file. This will allow us to map each sensor's data to its specific location.

Firstly we'll import the site index file and then we'll join the files on two columns. We'll need to use the merge function as the columns all have different names.

Let's open the site number listing file and check out the variables.

{% highlight py %}
sites = pd.read_excel(f'{rootdir}/VicRoads_Bike_Site_Number_Listing.xlsx')
sites.info()
{% endhighlight %}

RangeIndex: 93 entries, 0 to 92

|       Column | Non-Null Count |   Dtype |
|-------------:|---------------:|--------:|
|       ------ | -------------- |   ----- |
|      SITE_ID |    93 non-null |   int64 |
|       TFM_ID |    93 non-null |   int64 |
|     STRT_LAT |    93 non-null | float64 |
|    STRT_LONG |    93 non-null | float64 |
|          GPS |    92 non-null |  object |
|    SITE_NAME |    91 non-null |  object |
|     TFM_DESC |    93 non-null |  object |
| BEARING_DESC |    93 non-null | object  |
| DATA_SRC_CD  |    93 non-null | object  |
| RGN_SHORT_NM |    93 non-null | object  |
| Comments     |    75 non-null | object  |

It looks like there might be a few empty or null values in the site name, GPS and comments sections. Looking at the file, two of the sensors were decommissioned at the end of 2019, so we'll leave that data as is. 

Now we'll view the summary statistics to check the data distribution for any outliers

{% highlight py %}
sites.describe()
{% endhighlight %}

|       |   STRT_LAT |  STRT_LONG |
|------:|-----------:|-----------:|
| count |  93.000000 |  93.000000 |
|  mean | -37.832249 | 144.998457 |
|   std |   0.112396 |   0.082142 |
|   min | -38.507720 | 144.736270 |
|   25% | -37.832280 | 144.973640 |
|   50% | -37.806770 | 144.987550 |
|   75% | -37.783990 | 145.028100 |
|   max | -37.743220 | 145.291100 |

It's helpful to look at the min and max values, checking particularly for any data errors in the latitude and longitudes (eg. recording a positive rather than negative latitude). Everything is looking good though!
 
 
Let's make a quick map to see the locations seem correct.

{% highlight py %}
fig = px.scatter_mapbox(sites, lat="STRT_LAT", lon="STRT_LONG", hover_name="SITE_NAME", hover_data=["SITE_ID", "BEARING_DESC"],
                        color_discrete_sequence=["fuchsia"], zoom=10, height=300)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.write_html(f'{visuals}Site_Map.html')
fig.show()
{% endhighlight %}

<iframe src="/data_stories/visuals/bicycle_flows/Site_Map.html" height="300px" width="100%" style="border:none;"></iframe>


### And now to the merging

Now let's combine the sensor location information from the site index file, with all of the various sensor CSVs in which the bicycle volume and data are recorded.

We'll merge the 'SITE_XN_ROUTE' column with the 'SITE_ID' column, while the 'LOC_LEG' column will be merged with 'TFM_ID'

The 'SITE_ID' variable refers to the general site, while the 'TFM_ID' refers to individual sensors at that location.

Most sites have 2 individual sensors to measure bi-directional bicycle flows.

{% highlight py %}
df_merge = pd.merge(sites, df, left_on = ['SITE_ID','TFM_ID'], 
right_on = ['SITE_XN_ROUTE', 'LOC_LEG'])
{% endhighlight %}

### Recap
*So far we have:*

* confirmed that all the sensor data CSVs share the same variables
* combined each of the CSVs from 2017 to 2021 into a dataframe
* created a new aggregate variable called 'month_year'
* looked at the distribution of data in both the dataframe and the site index file
* merged the sensor data and the site index file to create a 'mappable' dataset

The next blog will focus on the analysis and visualisation of this dataset - thanks for following along!
